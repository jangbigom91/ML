{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba14546c",
   "metadata": {},
   "source": [
    "## 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6a842",
   "metadata": {},
   "source": [
    "#### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화 : sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.\\\n",
    "                You can see it out your window or on your television.\\\n",
    "                You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35bdf66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화 : word_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880cb3e",
   "metadata": {},
   "source": [
    "#### 문장별 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c4e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장들에 대한 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장들에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b997cc",
   "metadata": {},
   "source": [
    "## stopwords 제거\n",
    "### 불용어(Stop word)는 분석에 큰 의미가 없는 단어를 지칭합니다. \n",
    "### 예를 들어 the, a, an, is, I, my 등과 같이 문장을 구성하는 필수 요소지만 문맥적으로 큰 의미가 없는 단어가 이에 속합니다. \n",
    "### 이런 불용어는 텍스트에 빈번하게 나타나기 때문에 중요한 단어로 인지될 수 있습니다. 하지만 실질적으로는 중요한 단어가 아니므로 사전에 제거해줘야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a4617",
   "metadata": {},
   "source": [
    "#### 영어 스탑워즈 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f3b830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9568c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# 영어 stopwords 확인\n",
    "print('영어 stop words 갯수 : ', len(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# 영어 stopwords 중 10개만 출력해보자\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c0366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 위 예제 3개 문장에서 얻은 단어 토큰에 대해 stopwords 제거해보자\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제의 3개의 문장별로 얻은 workd_tokens list에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filterd_words = []\n",
    "    \n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환합니다.\n",
    "        word = word.lower()\n",
    "        \n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filterd_words.append(word)\n",
    "    \n",
    "    all_tokens.append(filterd_words)\n",
    "    \n",
    "print(all_tokens)                 # the, is 등 stopwords가 제거된 것 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21592e",
   "metadata": {},
   "source": [
    "## Stemming과 Lemmatization\n",
    "### 택스트 전처리의 목적은 말뭉치(Corpus)로부터 복잡성을 줄이는 것입니다. 어간 추출과 표제어 추출 역시 말뭉치의 복잡성을 줄여주는 텍스트 정규화 기법입니다.\n",
    "### 어간 추출(Stemming)과 표제어 추출(Lemmatization)은 단어의 원형을 찾는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c0057",
   "metadata": {},
   "source": [
    "#### Stemmer\n",
    "#### 어간 추출(Stemming)이란 단어로 부터 어간(Stem)을 추출하는 작업을 뜻합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ba7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b746f",
   "metadata": {},
   "source": [
    "#### Lemmatizer\n",
    "#### 어간 추출(Stemming)보다 표제어 추출(Lemmatization)이 더 정확히 어근 단어를 찾아줍니다.\n",
    "#### 표제어 추출은 WordNetLemmatizer를 주로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca0416d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff4df45",
   "metadata": {},
   "source": [
    "## 희소행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad11c72",
   "metadata": {},
   "source": [
    "### COO 형식\n",
    "#### Coordinate(좌표) 방식을 의미하며 0이 아닌 데이터만 별도의 배열에 저장하고 그 데이터를 가리키는 행과\n",
    "#### 열의 위치를 별도의 배열로 저장하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a5bb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdef34d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4403f",
   "metadata": {},
   "source": [
    "### CSR 형식\n",
    "#### COO 형식이 위치 배열값을 중복적으로 가지는 문제를 해결한 방식, 일반적으로 CSR형식이 COO보다 많이 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ad8bc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "              [1,4,0,3,2,5],\n",
    "              [0,6,0,3,0,0],\n",
    "              [2,0,0,0,0,0],\n",
    "              [0,0,0,7,0,8],\n",
    "              [1,0,0,0,0,0]])\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인')\n",
    "print(sparse_coo.toarray())\n",
    "\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca439e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
